{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"MNIST_dataset/mnist_train.csv\")\n",
    "test_data  = pd.read_csv(\"MNIST_dataset/mnist_test.csv\") \n",
    "Y_train = np.array(train_data.label)\n",
    "Y_test  = np.array(test_data.label)\n",
    "train_data.drop(columns=[\"label\"],inplace=True)\n",
    "test_data.drop(columns=[\"label\"],inplace=True)\n",
    "X_train = np.array(train_data)\n",
    "X_test = np.array(test_data)\n",
    "myscalar = StandardScaler()\n",
    "X_train = myscalar.fit_transform(X_train)\n",
    "X_test = myscalar.transform(X_test)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’},\n",
    "clf = MLPClassifier(hidden_layer_sizes=[256,128,64],activation=\"tanh\",random_state=1, max_iter=100,learning_rate_init=0.1,batch_size=128,verbose=1,solver=\"sgd\",alpha=0,momentum=0,tol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35222350\n",
      "Iteration 2, loss = 0.18144116\n",
      "Iteration 3, loss = 0.13189198\n",
      "Iteration 4, loss = 0.09985629\n",
      "Iteration 5, loss = 0.07700433\n",
      "Iteration 6, loss = 0.05942891\n",
      "Iteration 7, loss = 0.04603564\n",
      "Iteration 8, loss = 0.03547566\n",
      "Iteration 9, loss = 0.02728584\n",
      "Iteration 10, loss = 0.02092004\n",
      "Iteration 11, loss = 0.01629579\n",
      "Iteration 12, loss = 0.01293246\n",
      "Iteration 13, loss = 0.01033172\n",
      "Iteration 14, loss = 0.00823820\n",
      "Iteration 15, loss = 0.00692202\n",
      "Iteration 16, loss = 0.00584312\n",
      "Iteration 17, loss = 0.00500257\n",
      "Iteration 18, loss = 0.00428590\n",
      "Iteration 19, loss = 0.00377354\n",
      "Iteration 20, loss = 0.00336208\n",
      "Iteration 21, loss = 0.00302244\n",
      "Iteration 22, loss = 0.00274967\n",
      "Iteration 23, loss = 0.00250978\n",
      "Iteration 24, loss = 0.00229978\n",
      "Iteration 25, loss = 0.00212466\n",
      "Iteration 26, loss = 0.00198001\n",
      "Iteration 27, loss = 0.00184764\n",
      "Iteration 28, loss = 0.00172818\n",
      "Iteration 29, loss = 0.00162581\n",
      "Iteration 30, loss = 0.00153521\n",
      "Iteration 31, loss = 0.00145271\n",
      "Iteration 32, loss = 0.00138774\n",
      "Iteration 33, loss = 0.00131482\n",
      "Iteration 34, loss = 0.00124812\n",
      "Iteration 35, loss = 0.00119060\n",
      "Iteration 36, loss = 0.00114063\n",
      "Iteration 37, loss = 0.00109332\n",
      "Iteration 38, loss = 0.00104755\n",
      "Iteration 39, loss = 0.00102071\n",
      "Iteration 40, loss = 0.00096885\n",
      "Iteration 41, loss = 0.00093366\n",
      "Iteration 42, loss = 0.00089996\n",
      "Iteration 43, loss = 0.00086592\n",
      "Iteration 44, loss = 0.00084011\n",
      "Iteration 45, loss = 0.00081456\n",
      "Iteration 46, loss = 0.00078436\n",
      "Iteration 47, loss = 0.00076040\n",
      "Iteration 48, loss = 0.00074027\n",
      "Iteration 49, loss = 0.00071609\n",
      "Iteration 50, loss = 0.00069808\n",
      "Iteration 51, loss = 0.00067948\n",
      "Iteration 52, loss = 0.00065904\n",
      "Iteration 53, loss = 0.00064501\n",
      "Iteration 54, loss = 0.00062896\n",
      "Iteration 55, loss = 0.00061678\n",
      "Iteration 56, loss = 0.00059512\n",
      "Iteration 57, loss = 0.00057992\n",
      "Iteration 58, loss = 0.00056914\n",
      "Iteration 59, loss = 0.00055133\n",
      "Iteration 60, loss = 0.00054114\n",
      "Iteration 61, loss = 0.00053338\n",
      "Iteration 62, loss = 0.00052107\n",
      "Iteration 63, loss = 0.00050755\n",
      "Iteration 64, loss = 0.00050155\n",
      "Iteration 65, loss = 0.00049613\n",
      "Iteration 66, loss = 0.00047830\n",
      "Iteration 67, loss = 0.00047095\n",
      "Iteration 68, loss = 0.00046041\n",
      "Iteration 69, loss = 0.00045237\n",
      "Iteration 70, loss = 0.00044769\n",
      "Iteration 71, loss = 0.00044180\n",
      "Iteration 72, loss = 0.00042773\n",
      "Iteration 73, loss = 0.00042265\n",
      "Iteration 74, loss = 0.00041768\n",
      "Iteration 75, loss = 0.00040542\n",
      "Iteration 76, loss = 0.00040194\n",
      "Iteration 77, loss = 0.00039095\n",
      "Iteration 78, loss = 0.00038922\n",
      "Iteration 79, loss = 0.00037960\n",
      "Iteration 80, loss = 0.00037607\n",
      "Iteration 81, loss = 0.00036908\n",
      "Iteration 82, loss = 0.00036508\n",
      "Iteration 83, loss = 0.00036722\n",
      "Iteration 84, loss = 0.00035242\n",
      "Iteration 85, loss = 0.00035532\n",
      "Iteration 86, loss = 0.00034370\n",
      "Iteration 87, loss = 0.00033452\n",
      "Iteration 88, loss = 0.00033306\n",
      "Iteration 89, loss = 0.00032929\n",
      "Iteration 90, loss = 0.00032591\n",
      "Iteration 91, loss = 0.00032018\n",
      "Iteration 92, loss = 0.00032108\n",
      "Iteration 93, loss = 0.00031083\n",
      "Iteration 94, loss = 0.00030525\n",
      "Iteration 95, loss = 0.00031160\n",
      "Iteration 96, loss = 0.00030171\n",
      "Iteration 97, loss = 0.00029457\n",
      "Iteration 98, loss = 0.00029078\n",
      "Iteration 99, loss = 0.00028905\n",
      "Iteration 100, loss = 0.00029466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navneet/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0, batch_size=128, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=[256, 128, 64], learning_rate='constant',\n",
       "              learning_rate_init=0.1, max_fun=15000, max_iter=100, momentum=0,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='sgd', tol=0,\n",
       "              validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh 0.9672\n"
     ]
    }
   ],
   "source": [
    "print(\"tanh\",clf.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.36111342\n",
      "Iteration 2, loss = 0.13949632\n",
      "Iteration 3, loss = 0.08861919\n",
      "Iteration 4, loss = 0.06168184\n",
      "Iteration 5, loss = 0.04237097\n",
      "Iteration 6, loss = 0.03104103\n",
      "Iteration 7, loss = 0.02331966\n",
      "Iteration 8, loss = 0.01491682\n",
      "Iteration 9, loss = 0.00948006\n",
      "Iteration 10, loss = 0.00659388\n",
      "Iteration 11, loss = 0.00492091\n",
      "Iteration 12, loss = 0.00358140\n",
      "Iteration 13, loss = 0.00270862\n",
      "Iteration 14, loss = 0.00233204\n",
      "Iteration 15, loss = 0.00213250\n",
      "Iteration 16, loss = 0.00160253\n",
      "Iteration 17, loss = 0.00144891\n",
      "Iteration 18, loss = 0.00127324\n",
      "Iteration 19, loss = 0.00120704\n",
      "Iteration 20, loss = 0.00111223\n",
      "Iteration 21, loss = 0.00090005\n",
      "Iteration 22, loss = 0.00088399\n",
      "Iteration 23, loss = 0.00071380\n",
      "Iteration 24, loss = 0.00071507\n",
      "Iteration 25, loss = 0.00073349\n",
      "Iteration 26, loss = 0.00081387\n",
      "Iteration 27, loss = 0.00077552\n",
      "Iteration 28, loss = 0.00056608\n",
      "Iteration 29, loss = 0.00059317\n",
      "Iteration 30, loss = 0.00049497\n",
      "Iteration 31, loss = 0.00056183\n",
      "Iteration 32, loss = 0.00051088\n",
      "Iteration 33, loss = 0.00050072\n",
      "Iteration 34, loss = 0.00039972\n",
      "Iteration 35, loss = 0.00041292\n",
      "Iteration 36, loss = 0.00043089\n",
      "Iteration 37, loss = 0.00035163\n",
      "Iteration 38, loss = 0.00038780\n",
      "Iteration 39, loss = 0.00037543\n",
      "Iteration 40, loss = 0.00032137\n",
      "Iteration 41, loss = 0.00030041\n",
      "Iteration 42, loss = 0.00032294\n",
      "Iteration 43, loss = 0.00028144\n",
      "Iteration 44, loss = 0.00026960\n",
      "Iteration 45, loss = 0.00026509\n",
      "Iteration 46, loss = 0.00025094\n",
      "Iteration 47, loss = 0.00024905\n",
      "Iteration 48, loss = 0.00028052\n",
      "Iteration 49, loss = 0.00023364\n",
      "Iteration 50, loss = 0.00023189\n",
      "Iteration 51, loss = 0.00024034\n",
      "Iteration 52, loss = 0.00021410\n",
      "Iteration 53, loss = 0.00022909\n",
      "Iteration 54, loss = 0.00025843\n",
      "Iteration 55, loss = 0.00024901\n",
      "Iteration 56, loss = 0.00019045\n",
      "Iteration 57, loss = 0.00018604\n",
      "Iteration 58, loss = 0.00018398\n",
      "Iteration 59, loss = 0.00017921\n",
      "Iteration 60, loss = 0.00017506\n",
      "Iteration 61, loss = 0.00016796\n",
      "Iteration 62, loss = 0.00016486\n",
      "Iteration 63, loss = 0.00015955\n",
      "Iteration 64, loss = 0.00016672\n",
      "Iteration 65, loss = 0.00016016\n",
      "Iteration 66, loss = 0.00014883\n",
      "Iteration 67, loss = 0.00014799\n",
      "Iteration 68, loss = 0.00014580\n",
      "Iteration 69, loss = 0.00014000\n",
      "Iteration 70, loss = 0.00018709\n",
      "Iteration 71, loss = 0.00015993\n",
      "Iteration 72, loss = 0.00013581\n",
      "Iteration 73, loss = 0.00013901\n",
      "Iteration 74, loss = 0.00014091\n",
      "Iteration 75, loss = 0.00015496\n",
      "Iteration 76, loss = 0.00014126\n",
      "Iteration 77, loss = 0.00011877\n",
      "Iteration 78, loss = 0.00012050\n",
      "Iteration 79, loss = 0.00011074\n",
      "Iteration 80, loss = 0.00011057\n",
      "Iteration 81, loss = 0.00014236\n",
      "Iteration 82, loss = 0.00016497\n",
      "Iteration 83, loss = 0.00016065\n",
      "Iteration 84, loss = 0.00013216\n",
      "Iteration 85, loss = 0.00013988\n",
      "Iteration 86, loss = 0.00010574\n",
      "Iteration 87, loss = 0.00010025\n",
      "Iteration 88, loss = 0.00010067\n",
      "Iteration 89, loss = 0.00009821\n",
      "Iteration 90, loss = 0.00009891\n",
      "Iteration 91, loss = 0.00010333\n",
      "Iteration 92, loss = 0.00009530\n",
      "Iteration 93, loss = 0.00008814\n",
      "Iteration 94, loss = 0.00008462\n",
      "Iteration 95, loss = 0.00008326\n",
      "Iteration 96, loss = 0.00008068\n",
      "Iteration 97, loss = 0.00007896\n",
      "Iteration 98, loss = 0.00007783\n",
      "Iteration 99, loss = 0.00007661\n",
      "Iteration 100, loss = 0.00007541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navneet/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu 0.9733\n"
     ]
    }
   ],
   "source": [
    "# activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’},\n",
    "relu = MLPClassifier(hidden_layer_sizes=[256,128,64],activation=\"relu\",random_state=1, max_iter=100,learning_rate_init=0.1,batch_size=128,verbose=1,solver=\"sgd\",alpha=0,momentum=0,tol=0)\n",
    "relu.fit(x_train,y_train)\n",
    "print(\"relu\",relu.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.29790586\n",
      "Iteration 2, loss = 2.21919353\n",
      "Iteration 3, loss = 1.62923782\n",
      "Iteration 4, loss = 0.97356270\n",
      "Iteration 5, loss = 0.69625419\n",
      "Iteration 6, loss = 0.55191779\n",
      "Iteration 7, loss = 0.46635698\n",
      "Iteration 8, loss = 0.41704721\n",
      "Iteration 9, loss = 0.38010878\n",
      "Iteration 10, loss = 0.34820075\n",
      "Iteration 11, loss = 0.31878491\n",
      "Iteration 12, loss = 0.29187130\n",
      "Iteration 13, loss = 0.26797080\n",
      "Iteration 14, loss = 0.24806858\n",
      "Iteration 15, loss = 0.23059795\n",
      "Iteration 16, loss = 0.21527381\n",
      "Iteration 17, loss = 0.20130758\n",
      "Iteration 18, loss = 0.18914825\n",
      "Iteration 19, loss = 0.17745199\n",
      "Iteration 20, loss = 0.16694780\n",
      "Iteration 21, loss = 0.15710687\n",
      "Iteration 22, loss = 0.14747434\n",
      "Iteration 23, loss = 0.13961379\n",
      "Iteration 24, loss = 0.13165213\n",
      "Iteration 25, loss = 0.12398655\n",
      "Iteration 26, loss = 0.11723181\n",
      "Iteration 27, loss = 0.11085458\n",
      "Iteration 28, loss = 0.10445776\n",
      "Iteration 29, loss = 0.09919111\n",
      "Iteration 30, loss = 0.09379272\n",
      "Iteration 31, loss = 0.08851883\n",
      "Iteration 32, loss = 0.08369437\n",
      "Iteration 33, loss = 0.07905433\n",
      "Iteration 34, loss = 0.07476734\n",
      "Iteration 35, loss = 0.07062316\n",
      "Iteration 36, loss = 0.06656954\n",
      "Iteration 37, loss = 0.06306312\n",
      "Iteration 38, loss = 0.05963966\n",
      "Iteration 39, loss = 0.05638700\n",
      "Iteration 40, loss = 0.05301726\n",
      "Iteration 41, loss = 0.05033684\n",
      "Iteration 42, loss = 0.04737318\n",
      "Iteration 43, loss = 0.04470648\n",
      "Iteration 44, loss = 0.04225816\n",
      "Iteration 45, loss = 0.03991668\n",
      "Iteration 46, loss = 0.03778711\n",
      "Iteration 47, loss = 0.03568301\n",
      "Iteration 48, loss = 0.03359094\n",
      "Iteration 49, loss = 0.03169247\n",
      "Iteration 50, loss = 0.03018981\n",
      "Iteration 51, loss = 0.02846177\n",
      "Iteration 52, loss = 0.02699600\n",
      "Iteration 53, loss = 0.02551346\n",
      "Iteration 54, loss = 0.02408282\n",
      "Iteration 55, loss = 0.02288079\n",
      "Iteration 56, loss = 0.02169413\n",
      "Iteration 57, loss = 0.02059201\n",
      "Iteration 58, loss = 0.01957364\n",
      "Iteration 59, loss = 0.01864175\n",
      "Iteration 60, loss = 0.01772059\n",
      "Iteration 61, loss = 0.01687202\n",
      "Iteration 62, loss = 0.01609159\n",
      "Iteration 63, loss = 0.01527586\n",
      "Iteration 64, loss = 0.01456968\n",
      "Iteration 65, loss = 0.01392715\n",
      "Iteration 66, loss = 0.01332136\n",
      "Iteration 67, loss = 0.01273920\n",
      "Iteration 68, loss = 0.01217122\n",
      "Iteration 69, loss = 0.01165811\n",
      "Iteration 70, loss = 0.01111107\n",
      "Iteration 71, loss = 0.01065766\n",
      "Iteration 72, loss = 0.01021035\n",
      "Iteration 73, loss = 0.00981654\n",
      "Iteration 74, loss = 0.00944312\n",
      "Iteration 75, loss = 0.00902054\n",
      "Iteration 76, loss = 0.00866729\n",
      "Iteration 77, loss = 0.00832046\n",
      "Iteration 78, loss = 0.00800157\n",
      "Iteration 79, loss = 0.00771119\n",
      "Iteration 80, loss = 0.00745470\n",
      "Iteration 81, loss = 0.00714356\n",
      "Iteration 82, loss = 0.00689421\n",
      "Iteration 83, loss = 0.00664412\n",
      "Iteration 84, loss = 0.00639781\n",
      "Iteration 85, loss = 0.00619317\n",
      "Iteration 86, loss = 0.00600741\n",
      "Iteration 87, loss = 0.00580779\n",
      "Iteration 88, loss = 0.00560485\n",
      "Iteration 89, loss = 0.00546211\n",
      "Iteration 90, loss = 0.00528559\n",
      "Iteration 91, loss = 0.00510033\n",
      "Iteration 92, loss = 0.00496174\n",
      "Iteration 93, loss = 0.00482406\n",
      "Iteration 94, loss = 0.00467169\n",
      "Iteration 95, loss = 0.00455929\n",
      "Iteration 96, loss = 0.00442062\n",
      "Iteration 97, loss = 0.00430143\n",
      "Iteration 98, loss = 0.00417778\n",
      "Iteration 99, loss = 0.00406553\n",
      "Iteration 100, loss = 0.00395794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navneet/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid 0.9666\n"
     ]
    }
   ],
   "source": [
    "# activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’},\n",
    "logistic = MLPClassifier(hidden_layer_sizes=[256,128,64],activation=\"logistic\",random_state=1, max_iter=100,learning_rate_init=0.1,batch_size=128,verbose=1,solver=\"sgd\",alpha=0,momentum=0,tol=0)\n",
    "logistic.fit(x_train,y_train)\n",
    "print(\"sigmoid\",logistic.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.45818140\n",
      "Iteration 2, loss = 0.33342434\n",
      "Iteration 3, loss = 0.30349736\n",
      "Iteration 4, loss = 0.29412282\n",
      "Iteration 5, loss = 0.28656531\n",
      "Iteration 6, loss = 0.27545290\n",
      "Iteration 7, loss = 0.27533803\n",
      "Iteration 8, loss = 0.27100131\n",
      "Iteration 9, loss = 0.26908082\n",
      "Iteration 10, loss = 0.26480790\n",
      "Iteration 11, loss = 0.26313955\n",
      "Iteration 12, loss = 0.26018805\n",
      "Iteration 13, loss = 0.26237413\n",
      "Iteration 14, loss = 0.26177185\n",
      "Iteration 15, loss = 0.25785535\n",
      "Iteration 16, loss = 0.25596514\n",
      "Iteration 17, loss = 0.25160998\n",
      "Iteration 18, loss = 0.25214201\n",
      "Iteration 19, loss = 0.25044934\n",
      "Iteration 20, loss = 0.25371572\n",
      "Iteration 21, loss = 0.24879610\n",
      "Iteration 22, loss = 0.24885386\n",
      "Iteration 23, loss = 0.25025746\n",
      "Iteration 24, loss = 0.24737737\n",
      "Iteration 25, loss = 0.25068024\n",
      "Iteration 26, loss = 0.24358958\n",
      "Iteration 27, loss = 0.24571098\n",
      "Iteration 28, loss = 0.24481928\n",
      "Iteration 29, loss = 0.24434111\n",
      "Iteration 30, loss = 0.24564711\n",
      "Iteration 31, loss = 0.24657029\n",
      "Iteration 32, loss = 0.24329417\n",
      "Iteration 33, loss = 0.24404647\n",
      "Iteration 34, loss = 0.24204104\n",
      "Iteration 35, loss = 0.24402715\n",
      "Iteration 36, loss = 0.24302023\n",
      "Iteration 37, loss = 0.24273276\n",
      "Iteration 38, loss = 0.24054673\n",
      "Iteration 39, loss = 0.24092356\n",
      "Iteration 40, loss = 0.24023427\n",
      "Iteration 41, loss = 0.23891442\n",
      "Iteration 42, loss = 0.23783690\n",
      "Iteration 43, loss = 0.23967569\n",
      "Iteration 44, loss = 0.23727689\n",
      "Iteration 45, loss = 0.23882537\n",
      "Iteration 46, loss = 0.23832655\n",
      "Iteration 47, loss = 0.23843362\n",
      "Iteration 48, loss = 0.23769817\n",
      "Iteration 49, loss = 0.23801076\n",
      "Iteration 50, loss = 0.23948009\n",
      "Iteration 51, loss = 0.23744065\n",
      "Iteration 52, loss = 0.23866785\n",
      "Iteration 53, loss = 0.23822270\n",
      "Iteration 54, loss = 0.23633470\n",
      "Iteration 55, loss = 0.23661155\n",
      "Iteration 56, loss = 0.23506302\n",
      "Iteration 57, loss = 0.23432066\n",
      "Iteration 58, loss = 0.23738529\n",
      "Iteration 59, loss = 0.23480057\n",
      "Iteration 60, loss = 0.23489524\n",
      "Iteration 61, loss = 0.23544678\n",
      "Iteration 62, loss = 0.23427693\n",
      "Iteration 63, loss = 0.23555484\n",
      "Iteration 64, loss = 0.23469917\n",
      "Iteration 65, loss = 0.23348273\n",
      "Iteration 66, loss = 0.23509619\n",
      "Iteration 67, loss = 0.23279197\n",
      "Iteration 68, loss = 0.23510074\n",
      "Iteration 69, loss = 0.23441260\n",
      "Iteration 70, loss = 0.23298089\n",
      "Iteration 71, loss = 0.23291892\n",
      "Iteration 72, loss = 0.23411970\n",
      "Iteration 73, loss = 0.23335777\n",
      "Iteration 74, loss = 0.23448315\n",
      "Iteration 75, loss = 0.23335518\n",
      "Iteration 76, loss = 0.23267209\n",
      "Iteration 77, loss = 0.23206253\n",
      "Iteration 78, loss = 0.23416771\n",
      "Iteration 79, loss = 0.23287017\n",
      "Iteration 80, loss = 0.23107710\n",
      "Iteration 81, loss = 0.23208490\n",
      "Iteration 82, loss = 0.23349279\n",
      "Iteration 83, loss = 0.23108904\n",
      "Iteration 84, loss = 0.23128725\n",
      "Iteration 85, loss = 0.23178696\n",
      "Iteration 86, loss = 0.23086147\n",
      "Iteration 87, loss = 0.23242256\n",
      "Iteration 88, loss = 0.23007956\n",
      "Iteration 89, loss = 0.23045031\n",
      "Iteration 90, loss = 0.23072143\n",
      "Iteration 91, loss = 0.23076348\n",
      "Iteration 92, loss = 0.23158094\n",
      "Iteration 93, loss = 0.22994480\n",
      "Iteration 94, loss = 0.23006122\n",
      "Iteration 95, loss = 0.23136208\n",
      "Iteration 96, loss = 0.23043251\n",
      "Iteration 97, loss = 0.23125171\n",
      "Iteration 98, loss = 0.22992812\n",
      "Iteration 99, loss = 0.23080157\n",
      "Iteration 100, loss = 0.22904495\n",
      "linear 0.9197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navneet/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# activation{‘identity’, ‘logistic’, ‘tanh’, ‘relu’},\n",
    "identity = MLPClassifier(hidden_layer_sizes=[256,128,64],activation=\"identity\",random_state=1, max_iter=100,learning_rate_init=0.1,batch_size=128,verbose=1,solver=\"sgd\",alpha=0,momentum=0,tol=0)\n",
    "identity.fit(x_train,y_train)\n",
    "print(\"linear\",identity.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear\n",
      "Train 0.9380625\n",
      "Valid 0.9106666666666666\n",
      "Test 0.9197\n",
      "\n",
      "Relu\n",
      "Train 1.0\n",
      "Valid 0.9691666666666666\n",
      "Test 0.9733\n",
      "\n",
      "Tanh\n",
      "Train 0.9999791666666666\n",
      "Valid 0.9665\n",
      "Test 0.9672\n",
      "\n",
      "Sigmoid\n",
      "Train 0.9999166666666667\n",
      "Valid 0.9676666666666667\n",
      "Test 0.9666\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear\")\n",
    "print(\"Train\",identity.score(x_train,y_train))\n",
    "print(\"Valid\",identity.score(x_val,y_val))\n",
    "print(\"Test\",identity.score(X_test,Y_test))\n",
    "\n",
    "print(\"\\nRelu\")\n",
    "print(\"Train\",relu.score(x_train,y_train))\n",
    "print(\"Valid\",relu.score(x_val,y_val))\n",
    "print(\"Test\",relu.score(X_test,Y_test))\n",
    "\n",
    "print(\"\\nTanh\")\n",
    "print(\"Train\",clf.score(x_train,y_train))\n",
    "print(\"Valid\",clf.score(x_val,y_val))\n",
    "print(\"Test\",clf.score(X_test,Y_test))\n",
    "\n",
    "print(\"\\nSigmoid\")\n",
    "print(\"Train\",logistic.score(x_train,y_train))\n",
    "print(\"Valid\",logistic.score(x_val,y_val))\n",
    "print(\"Test\",logistic.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
